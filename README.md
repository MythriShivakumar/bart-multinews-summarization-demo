# Multi-News Summarization using BART + Gradio Demo

This project fine-tunes a BART model on the **Multi-News** dataset for multi-document summarization and deploys it using **Gradio** for live inference. It demonstrates the use of Hugging Face Transformers and Datasets libraries for end-to-end model training and deployment.

## ğŸ“° Dataset

- **Multi-News**: A dataset of clusters of news articles with their human-written summaries
- Provided by Hugging Face `datasets` library

## ğŸ§  Model

- **Base Model:** `facebook/bart-base`
- Tokenized with padding/truncation using BART tokenizer
- Fine-tuned for summarization using `Seq2SeqTrainer`
- Inference deployed via **Gradio** interface

## ğŸ”§ Preprocessing

- `document` â†’ Input to encoder (truncated to 1024 tokens)
- `summary` â†’ Target output (truncated to 256 tokens)
- Tokenized using Hugging Face tokenizer and mapped into dataset

## ğŸ“Š Evaluation

- ROUGE Score (`rouge1`, `rouge2`, `rougeL`)
- Computed using `evaluate` library

## ğŸŒ Web Demo

- Gradio interface allows real-time article summarization from user input
- Run locally or deploy via Colab

## ğŸ§° Requirements

```bash
pip install datasets transformers evaluate sacrebleu bert_score rouge-score gradio
